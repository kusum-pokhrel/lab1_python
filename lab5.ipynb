{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef142573",
   "metadata": {},
   "source": [
    "LAB REPORT\n",
    "Web Scraping using Python (Requests & BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaf687",
   "metadata": {},
   "source": [
    "Objective\n",
    "\n",
    "To practice web scraping techniques using Python libraries requests, BeautifulSoup, and pandas, including HTML parsing, navigation, table scraping, selectors, and exception handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08618",
   "metadata": {},
   "source": [
    "Tools & Libraries Used :\n",
    "- Python 3\n",
    "- requests\n",
    "- bs4 (BeautifulSoup)\n",
    "- pandas\n",
    "- csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be2d06",
   "metadata": {},
   "source": [
    "Theory\n",
    "\n",
    "Web scraping is the process of automatically extracting information from websites using programming techniques. Websites are generally written in HTML, which consists of elements such as tags, attributes, and content. Python provides powerful libraries to access and process web data efficiently.\n",
    "\n",
    "The requests library is used to send HTTP requests to a web server and retrieve the HTML content of a webpage. It supports features such as handling HTTP responses, timeouts, and network-related exceptions.\n",
    "\n",
    "BeautifulSoup, from the bs4 module, is used to parse HTML documents and convert them into a structured tree format. It allows easy searching, navigation, and modification of HTML elements using methods like find(), find_all(), and CSS selectors.\n",
    "\n",
    "Web scraping often involves navigating through HTML tags, extracting specific data such as links, headings, and tables, and storing the extracted data in files like CSV for further analysis. Proper exception handling is important to manage errors such as connection failures, invalid responses, or missing elements during scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d16e3c",
   "metadata": {},
   "source": [
    "Question 1: Basic HTML Request & Parsing\n",
    "Aim\n",
    "\n",
    "Fetch HTML content from GeeksforGeeks and print the <title> of the webpage while handling exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8773614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: GeeksforGeeks | Your All-in-One Learning Portal\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.geeksforgeeks.org\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()   # Raises HTTPError for bad responses\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    print(\"Page Title:\", soup.title.text)\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(\"HTTP Error:\", e)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Network Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352a75e",
   "metadata": {},
   "source": [
    "Question 2: Extract Links\n",
    "Aim\n",
    "\n",
    "Extract and print the first 5 hyperlinks using .find() and .find_all()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8072b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 hyperlinks:\n",
      " - https://www.geeksforgeeks.org/\n",
      "DSA - https://www.geeksforgeeks.org/dsa/dsa-tutorial-learn-data-structures-and-algorithms/\n",
      "Practice Problems - https://www.geeksforgeeks.org/explore\n",
      "C - https://www.geeksforgeeks.org/c/c-programming-language/\n",
      "C++ - https://www.geeksforgeeks.org/cpp/c-plus-plus/\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\", limit=5)\n",
    "\n",
    "print(\"First 5 hyperlinks:\")\n",
    "for link in links:\n",
    "    print(link.get_text(strip=True), \"-\", link.get(\"href\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934375f8",
   "metadata": {},
   "source": [
    "Question 3: Extract Headings & Save to CSV\n",
    "Aim\n",
    "- Extract all <h2> headings\n",
    "- Extract all <a> tags\n",
    "- Save headings to headings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c236cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headings saved to headings.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Extract h2 headings\n",
    "headings = [h.text.strip() for h in soup.find_all(\"h2\")]\n",
    "\n",
    "# Extract all links\n",
    "all_links = [a.get(\"href\") for a in soup.find_all(\"a\")]\n",
    "\n",
    "# Save headings to CSV\n",
    "with open(\"headings.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Headings\"])\n",
    "    for h in headings:\n",
    "        writer.writerow([h])\n",
    "\n",
    "print(\"Headings saved to headings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d630554",
   "metadata": {},
   "source": [
    "Question 4: Scrape Wikipedia Table\n",
    "Aim\n",
    "\n",
    "Scrape the first table from Wikipedia â€“ List of countries by population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d45ace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error: 403 Client Error: Forbidden for url: https://en.wikipedia.org/wiki/List_of_countries_by_population\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_population\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", class_=\"wikitable\")\n",
    "\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        cells = [cell.text.strip() for cell in row.find_all([\"td\", \"th\"])]\n",
    "        print(cells)\n",
    "\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"Request timed out\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(\"HTTP Error:\", e)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Request Error:\", e)\n",
    "except AttributeError:\n",
    "    print(\"Table not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aeb0ce",
   "metadata": {},
   "source": [
    "Question 5: Selectors & Navigation\n",
    "HTML Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c398465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intro Paragraphs: ['Welcome', 'Learn Python']\n",
      "Parent of <a>: body\n",
      "Next sibling: Learn Python\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<p class=\"intro\">Welcome</p>\n",
    "<p class=\"intro\">Learn Python</p>\n",
    "<a href=\"https://python.org\">Python</a>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract <p> tags with class intro\n",
    "intro_paragraphs = soup.find_all(\"p\", class_=\"intro\")\n",
    "print(\"Intro Paragraphs:\", [p.text for p in intro_paragraphs])\n",
    "\n",
    "# Parent of <a> tag\n",
    "a_tag = soup.find(\"a\")\n",
    "print(\"Parent of <a>:\", a_tag.parent.name)\n",
    "\n",
    "# Next sibling of first <p>\n",
    "print(\"Next sibling:\", intro_paragraphs[0].find_next_sibling().text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f1e08",
   "metadata": {},
   "source": [
    "Question 6: Tag Manipulation\n",
    "Aim\n",
    "\n",
    "Modify <b class=\"boldest\">Hello</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d82bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong class=\"boldest\" id=\"greeting\">Hi there</strong>\n"
     ]
    }
   ],
   "source": [
    "html = '<b class=\"boldest\">Hello</b>'\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tag = soup.b\n",
    "tag.name = \"strong\"\n",
    "tag[\"id\"] = \"greeting\"\n",
    "tag.string = \"Hi there\"\n",
    "\n",
    "print(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a80dd9",
   "metadata": {},
   "source": [
    "Question 7: Advanced Navigation\n",
    "HTML Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5dedc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent <td>: <td>Apple</td>\n",
      "Siblings: []\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<table>\n",
    "<tr><td>Apple</td></tr>\n",
    "<tr><td>Banana</td></tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find string \"Apple\"\n",
    "apple = soup.find(string=\"Apple\")\n",
    "print(\"Parent <td>:\", apple.parent)\n",
    "\n",
    "# Siblings of first <td>\n",
    "siblings = apple.parent.find_next_siblings()\n",
    "print(\"Siblings:\", siblings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc04950",
   "metadata": {},
   "source": [
    "Question 8: Using SoupStrainer\n",
    "Aim\n",
    "\n",
    "Parse only <a> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4419b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"page1.html\">\n",
      " Page 1\n",
      "</a>\n",
      "<a href=\"page2.html\">\n",
      " Page 2\n",
      "</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import SoupStrainer\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<a href=\"page1.html\">Page 1</a>\n",
    "<p>Paragraph</p>\n",
    "<a href=\"page2.html\">Page 2</a>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "only_a_tags = SoupStrainer(\"a\")\n",
    "soup = BeautifulSoup(html, \"html.parser\", parse_only=only_a_tags)\n",
    "\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ab726",
   "metadata": {},
   "source": [
    "Question 9: Exception Handling (Enhanced)\n",
    "Handled Exceptions :\n",
    "- Timeout\n",
    "- HTTPError\n",
    "- RequestException\n",
    "- AttributeError (table missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b5a907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    \n",
    "    if table is None:\n",
    "        raise AttributeError(\"Table not found\")\n",
    "\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"Timeout occurred\")\n",
    "except requests.exceptions.HTTPError:\n",
    "    print(\"HTTP error occurred\")\n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"Request exception occurred\")\n",
    "except AttributeError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af4361",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this lab, web scraping was successfully performed using Python libraries such as requests and BeautifulSoup. The program demonstrated how to fetch and parse HTML content, extract useful data like titles, links, headings, and tables, and navigate through HTML elements efficiently. Exception handling techniques were also implemented to manage errors such as network issues and missing elements. This lab helped in understanding the practical application of web scraping and enhanced skills in data extraction and HTML parsing using Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
